{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words\n",
    "\n",
    "## Step 1:\n",
    "text ='Deep learning methods are popular for natural language, primarily because\n",
    "they are delivering on their promise. Some of the first large demonstrations of the power of deep learning were in natural language processing, specifically speech recognition. More recently in machine translation. '\n",
    "\n",
    "We will first preprocess the data, in order to:\n",
    "- Convert text to lower case.\n",
    "- Remove all non-word characters.\n",
    "- Remove all punctuations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re \n",
    "import numpy as np \n",
    "\n",
    "text ='Deep learning methods are popular for natural language, primarily because they are delivering on their promise. Some of the first large demonstrations of the power of deep learning were in natural language processing, specifically speech recognition. More recently in machine translation. '\n",
    "dataset = nltk.sent_tokenize(text) \n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower() \n",
    "    dataset[i] = re.sub(r'\\W', ' ', dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+', ' ', dataset[i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can further preprocess the text to suit you needs.\n",
    "\n",
    "## Step #2 : Obtaining most frequent words in our text.\n",
    "\n",
    "We will apply the following steps to generate our model.\n",
    "\t- We declare a dictionary to hold our bag of words.\n",
    "\t- Next we tokenize each sentence to words.\n",
    "\t- Now for each word in a sentence, we check if the word exists in our dictionary.\n",
    "\t- If it does, then we increment its count by 1. If it doesnâ€™t, we add it to our dictionary and set its count as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {} \n",
    "for data in dataset: \n",
    "    words = nltk.word_tokenize(data) \n",
    "    for word in words: \n",
    "        if word not in word2count.keys(): \n",
    "            word2count[word] = 1\n",
    "        else: \n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model, we have a total of 41 words. However when processing large texts, the number of words could reach millions. We do not need to use all those words. Hence, we select a particular number of most frequently used words. To implement this we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of', 'deep', 'learning', 'are', 'natural', 'language', 'the', 'in', 'methods', 'popular', 'for', 'primarily', 'because', 'they', 'delivering', 'on', 'their', 'promise', 'some', 'first']\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "freq_words = heapq.nlargest(20, word2count, key=word2count.get)\n",
    "print(freq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where 20 denotes the number of words we want. If our text is large, we feed in a larger number.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step #3 : Building the Bag of Words model\n",
    "\n",
    "In this step we construct a vector, which would tell us whether a word in each sentence is a frequent word or not. If a word in a sentence is a frequent word, we set it as 1, else we set it as 0.\n",
    "\n",
    "This can be implemented with the help of following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0]\n",
      " [1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for data in dataset:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    X.append(vector)\n",
    "X = np.asarray(X)\n",
    "\n",
    "print(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
